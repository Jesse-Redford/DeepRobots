{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRobots Research Meeting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "\n",
    "### Discrete Implementation Details\n",
    "\n",
    "- state space (done)\n",
    "    - self.state\n",
    "    - (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "- action space\n",
    "    - joint velocities (adot1, adot2), range = (-pi/8, pi/8) not including the case where (adot1 = 0, adot2 = 0), intervals = 0.01\n",
    "    - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "- reward\n",
    "    - negative reward for singularity (a1 = a2) = -(link_length)*10\n",
    "    - negative reward based on proximity of a1 and a2 (using log function, log 0 = -inf)\n",
    "    - body_v[0] / (a1dot^2 + a2dot^2) * some constant for scaling\n",
    "        - displacement from current state to next state along x axis is x, x dot is velocity\n",
    "- transition model (done)\n",
    "- goal state (no goal state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "1. dicrete RL implementation\n",
    "2. Deep RL\n",
    "3. continuous implementation (python function sovler to solve ODE)\n",
    "4. DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6/5\n",
    "- To-do\n",
    "    - limit joint space same as wheeled robot\n",
    "    - compare good action vs current most common action using\n",
    "        - fixed policy q-learning (instead of choosing actions, roll out fixed policy) to compare q-values\n",
    "        - compare reward resulted by both\n",
    "        - compare learning curve as well\n",
    "    - show learning curve graph (average reward over time)\n",
    "    - run DQN for a day\n",
    "    - try linear epsilon decay, ending at 0.3\n",
    "    - come up with some other algorithms to run\n",
    "- Things to discuss\n",
    "    - has confirmed that the bad results look a lot like the results of running very few episodes (for example 2 episodes)\n",
    "    - 500000 iterations results\n",
    "    - Other algorithms\n",
    "        - PPO\n",
    "        - DDPG\n",
    "        - Other policy gradient algorithms\n",
    "    - Ciocarlie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5/3\n",
    "- To-do (procedure)\n",
    "    - hold both joint input angles fixed, and see what happens\n",
    "    - try quad, trapz, cumtrapz, etc. to decrease error\n",
    "    - for larger joint inputs, try to integrate more frequently\n",
    "    - See photo on phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/26\n",
    "- Questions & Observations\n",
    "    - make ROS or some other visual implementations?\n",
    "    - weight divergence when I make the action space more granular\n",
    "        - https://www.reddit.com/r/MachineLearning/comments/4hml3e/what_causes_qfunction_to_diverge_and_how_to/\n",
    "        - reward clipping\n",
    "        - Q_target clipping?\n",
    "        - Use C parameter in paper?\n",
    "    - what is k? differential viscous drag constant?\n",
    "        - k=1 for now\n",
    "    - computational costly to find inverse for swimming robot\n",
    "    - swimming robot testing\n",
    "    - movie?\n",
    "- To-do (procedure)\n",
    "    - both a1 and a2 for swimming robot is -pi/2 to pi/2\n",
    "    - fix wheeled robot implementation\n",
    "        - a1, a2 should be function of time (see picture, where phi = a)\n",
    "        - integrate eksi_matrix(t) = (jacobian(t) * joint inputs) for dt, instead of multiplying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/12\n",
    "- Questions & Observations\n",
    "    - a1 and a2 taking different time complicates reward calculations (a1dot, a2dot, body_v...)\n",
    "    - reward should be inertial_x_v instead of body_x_v!!! May have to rerun previous trials\n",
    "    - get rid of theta from state space?\n",
    "    - problem with local minimum  \n",
    "- To-do (procedure)\n",
    "    - | to remove denominator from reward\n",
    "    - | if either joint does not move, apply -5 reward\n",
    "    - | try a lot of iterations (50000+)\n",
    "    - | add try-except to catch zero-division errors\n",
    "    - remove theta from state space and use body_v_x velocity as reward\n",
    "        - first without limiting theta range, and see what we get\n",
    "        - if robot moves in a circle, then try limiting theta range (pi/4, -pi/4)\n",
    "    - When the any of the joint is at the joint limit state, assign -5 reward\n",
    "    - Change number of weights to 5000\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4/5\n",
    "- Questions & Observations\n",
    "    - velocity vs acceleration as action space?\n",
    "    - why does DQN like to choose actions that set one of the join velocity to 0?\n",
    "        - ![title](images/img1.png)\n",
    "    - Will enforcing angle limits during robot movements artificially create good actions?\n",
    "- To-do\n",
    "    - If the robot stays at the same place: then assign -1 reward (done)\n",
    "    - For limiting angle during robot movements, a1 and a2 should take different time if they both hit limits\n",
    "    - make the angle movements smaller (smaller t_interval)\n",
    "    - outline of presentation\n",
    "    - book flights (arrive Monday morning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/29\n",
    "\n",
    "- Questions & Observations\n",
    "    - Should I use Temporal Difference learning given I am using gradient descent?\n",
    "        - No\n",
    "    - model.fit vs model.train_on_batch\n",
    "        - figure out one-step first, and then try model.fit later\n",
    "    - What do we need to print out during DQN?\n",
    "    - What would be a good threshold in terms of number of elements in the memory for DQN to start sampling minibatches? batch_size? batch_size * 2?\n",
    "    - If an action makes a1 or a2 go out of (-pi/2, pi/2) range, that action should also be discarded right?\n",
    "    - parameters:\n",
    "        - singularity check: a1-a2 <= 0.00001?\n",
    "        - epsilon, epsilon_min, epsilon_decay?\n",
    "- To-do\n",
    "    - implement swimming robot dqn\n",
    "    - try out different parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/8\n",
    "\n",
    "- Questions & Observations\n",
    "    - For discrete RL, would it be worth it to change alpha or epsilon into functions?\n",
    "        - maybe not because q space is sparse, we want to keep exploring\n",
    "    - For discrete RL, would it make a difference if I end with learning or functional approximation?\n",
    "        - if we run it for long enough, it shouldn't\n",
    "    - (SOLVED)IMPORTANT: reason for RL to get stuck after functional approximation -> after some investigation, I discovered the problem. After functional approximation, the one action with the largest q value (approximated by function) may lead to illegal state transition, resulting in an infinite while loop of attempting to find the best action among only one available action\n",
    "    - realistically, is 0.01 second a good time interval?\n",
    "        - we can try smaller time intervals\n",
    "    - When I make the angle interval too granular, there will be millions of Q values, and Q value predictions take way too long to complete\n",
    "    - After some experimentation, I found the best architecture is a funnel with wide base network (kinda like a pyramid)\n",
    "    - DQN: P6 Of paper, marked with blue highlighter. Does it mean we use states as input and (action, Q) as outputs? Should we do this or stick with old version? In the description of the algorithm, they seem to use Q value only as the output.\n",
    "        - ignore this for now\n",
    "    - Found good DQN code for reference: https://github.com/keon/deep-q-learning/blob/master/dqn.py\n",
    "    - DQN: we can still use Keras, using model.compile allows us to initialize random weights, and using model.train_on_batch allow us to do a single gradient descent on a given batch of data: https://keras.io/models/sequential/\n",
    "- To-do\n",
    "    - change NN structure to 2 sided pyramind (2 funnels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/1\n",
    "\n",
    "- Questions\n",
    "    - About the paper\n",
    "    - Should I make a continuous version of robot for next step?\n",
    "- To-do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/15\n",
    "\n",
    "- To-do\n",
    "    - read the DQN paper\n",
    "    - change my NN implementation based on paper/future discussions\n",
    "    - experiment with opening up the ranges of theta, adots or making the intervals smaller\n",
    "    - make a function that iteratively does QLearning-RandomForest/NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/8\n",
    "\n",
    "- Updates\n",
    "    - implemented 3d graph of a1,a2 vs time\n",
    "        - ![title](images/Q_monitor.png)\n",
    "        - ![title](images/Q_3D_monitor.png)\n",
    "    - implement Fourier transformation\n",
    "        - ![title](images/Fourier Transformation.png)\n",
    "    \n",
    "- questions/problems:\n",
    "- To-do\n",
    "    - Implement a neural network that maps state + action space to Q values using only Q values that have been updated\n",
    "    - the current Q value matrix is fine to use\n",
    "    - don't need the test or validate, just focus on training accuracy\n",
    "    - X = {a1dot, a2dot, theta, a1, a2}, y = Q value\n",
    "    - If I have time:\n",
    "        - try compare: 10 robot q-learning and then NN vs 5 robot q-learning -> NN -> 5 robot learning again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/1\n",
    "\n",
    "- questions/problems:\n",
    "    - apologize for cumulating the work until thursday, will try to work on weekends + thursday from now on\n",
    "    - q-learning sometimes stuck in an infinite loop choosing an action?\n",
    "    - does not cover enough states so cannot do policy roll out?\n",
    "- To-do\n",
    "    1. implement\n",
    "        - Plot trajectories alpha1-alpha2 space\n",
    "        - Extract modes from trajectories\n",
    "            - Fourier transformations\n",
    "    2. play around(optional, for fun) \n",
    "        - Start each robot at same configuration (can try this)\n",
    "        - increase granularity (make angle interval smaller)\n",
    "        - Try limiting theta (-pi/4, pi/4, pi/32)\n",
    "        - change the reward to prioritize theta_dot (so that it turns in circles)\n",
    "    3. Function approximation for x (if have time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/25\n",
    "\n",
    "- questions:\n",
    "    - how to resolve weird state parameters in RL?\n",
    "- To-do\n",
    "   - change state space to (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "   - when theta goes out of range, in move(), add or minus 2pi depending on whether it is positive or negative\n",
    "   - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "   - implement graphing of x-velocity, x displacement, joint angles etc. in policy testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/27\n",
    "\n",
    "- questions:\n",
    "    - is my model working correctly?\n",
    "- observation:\n",
    "    - time_interval is dependent on angle_interval, if t_interval is too small, and change in angle_interval is too small, there might not be any update\n",
    "    - afraid that this might be a problem?\n",
    "- Deep RL:\n",
    "    - loss function = expected reward - groundtruth reward\n",
    "- RL implementation:\n",
    "    - always use epsilon-greedy approach\n",
    "     - epsilon should be something that decreases over time\n",
    "    - (optional)in general, joint velocities are sinusoidal -> this is how we should let the robot choose \n",
    "       - Tony will think about this more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/6\n",
    "\n",
    "- next time\n",
    "    - look at Atari paper for Deep RL implementation details\n",
    "- questions:\n",
    "    - what should self.state contain?\n",
    "    - implement discretized angles?\n",
    "    - are mutator/accessor methods useless?\n",
    "    - Deep RL link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/30\n",
    "\n",
    "- next time\n",
    "    - fix DeepRobots implementation\n",
    "    - ask Tony for Mathematica inputs and outputs\n",
    "    - compare with Python model\n",
    "- questions:\n",
    "    - openAI (don't worry)\n",
    "    - how to keep the continuity Scott mentioned (don't worry)\n",
    "    - this implementation is only for proximal link, do we need other links? (implement the function)\n",
    "    - do I need to integrate over theta_dot? Is it already given by x and y dot? (No)\n",
    "    - when the input is pi/3, pi/3, D becomes 0 (singularity)\n",
    "    - self.state is broken (another version of implementation) \n",
    "    - plot graph with matplotlib (noted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
