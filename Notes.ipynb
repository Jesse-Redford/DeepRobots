{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRobots Research Meeting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "\n",
    "### Discrete Implementation Details\n",
    "\n",
    "- state space (done)\n",
    "    - self.state\n",
    "    - (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "- action space\n",
    "    - joint velocities (adot1, adot2), range = (-pi/8, pi/8) not including the case where (adot1 = 0, adot2 = 0), intervals = 0.01\n",
    "    - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "- reward\n",
    "    - negative reward for singularity (a1 = a2) = -(link_length)*10\n",
    "    - negative reward based on proximity of a1 and a2 (using log function, log 0 = -inf)\n",
    "    - body_v[0] / (a1dot^2 + a2dot^2) * some constant for scaling\n",
    "        - displacement from current state to next state along x axis is x, x dot is velocity\n",
    "- transition model (done)\n",
    "- goal state (no goal state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "1. dicrete RL implementation\n",
    "2. Deep RL\n",
    "3. continuous implementation (python function sovler to solve ODE)\n",
    "4. DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/29\n",
    "\n",
    "- Questions & Observations\n",
    "    - Should I use Temporal Difference learning given I am using gradient descent?\n",
    "        - No\n",
    "    - model.fit vs model.train_on_batch\n",
    "        - figure out one-step first, and then try model.fit later\n",
    "    - What do we need to print out during DQN?\n",
    "    - What would be a good threshold in terms of number of elements in the memory for DQN to start sampling minibatches? batch_size? batch_size * 2?\n",
    "    - If an action makes a1 or a2 go out of (-pi/2, pi/2) range, that action should also be discarded right?\n",
    "    - parameters:\n",
    "        - singularity check: a1-a2 <= 0.00001?\n",
    "        - epsilon, epsilon_min, epsilon_decay?\n",
    "- To-do\n",
    "    - implement swimming robot dqn\n",
    "    - try out different parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/8\n",
    "\n",
    "- Questions & Observations\n",
    "    - For discrete RL, would it be worth it to change alpha or epsilon into functions?\n",
    "        - maybe not because q space is sparse, we want to keep exploring\n",
    "    - For discrete RL, would it make a difference if I end with learning or functional approximation?\n",
    "        - if we run it for long enough, it shouldn't\n",
    "    - (SOLVED)IMPORTANT: reason for RL to get stuck after functional approximation -> after some investigation, I discovered the problem. After functional approximation, the one action with the largest q value (approximated by function) may lead to illegal state transition, resulting in an infinite while loop of attempting to find the best action among only one available action\n",
    "    - realistically, is 0.01 second a good time interval?\n",
    "        - we can try smaller time intervals\n",
    "    - When I make the angle interval too granular, there will be millions of Q values, and Q value predictions take way too long to complete\n",
    "    - After some experimentation, I found the best architecture is a funnel with wide base network (kinda like a pyramid)\n",
    "    - DQN: P6 Of paper, marked with blue highlighter. Does it mean we use states as input and (action, Q) as outputs? Should we do this or stick with old version? In the description of the algorithm, they seem to use Q value only as the output.\n",
    "        - ignore this for now\n",
    "    - Found good DQN code for reference: https://github.com/keon/deep-q-learning/blob/master/dqn.py\n",
    "    - DQN: we can still use Keras, using model.compile allows us to initialize random weights, and using model.train_on_batch allow us to do a single gradient descent on a given batch of data: https://keras.io/models/sequential/\n",
    "- To-do\n",
    "    - change NN structure to 2 sided pyramind (2 funnels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3/1\n",
    "\n",
    "- Questions\n",
    "    - About the paper\n",
    "    - Should I make a continuous version of robot for next step?\n",
    "- To-do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/15\n",
    "\n",
    "- To-do\n",
    "    - read the DQN paper\n",
    "    - change my NN implementation based on paper/future discussions\n",
    "    - experiment with opening up the ranges of theta, adots or making the intervals smaller\n",
    "    - make a function that iteratively does QLearning-RandomForest/NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/8\n",
    "\n",
    "- Updates\n",
    "    - implemented 3d graph of a1,a2 vs time\n",
    "        - ![title](images/Q_monitor.png)\n",
    "        - ![title](images/Q_3D_monitor.png)\n",
    "    - implement Fourier transformation\n",
    "        - ![title](images/Fourier Transformation.png)\n",
    "    \n",
    "- questions/problems:\n",
    "- To-do\n",
    "    - Implement a neural network that maps state + action space to Q values using only Q values that have been updated\n",
    "    - the current Q value matrix is fine to use\n",
    "    - don't need the test or validate, just focus on training accuracy\n",
    "    - X = {a1dot, a2dot, theta, a1, a2}, y = Q value\n",
    "    - If I have time:\n",
    "        - try compare: 10 robot q-learning and then NN vs 5 robot q-learning -> NN -> 5 robot learning again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/1\n",
    "\n",
    "- questions/problems:\n",
    "    - apologize for cumulating the work until thursday, will try to work on weekends + thursday from now on\n",
    "    - q-learning sometimes stuck in an infinite loop choosing an action?\n",
    "    - does not cover enough states so cannot do policy roll out?\n",
    "- To-do\n",
    "    1. implement\n",
    "        - Plot trajectories alpha1-alpha2 space\n",
    "        - Extract modes from trajectories\n",
    "            - Fourier transformations\n",
    "    2. play around(optional, for fun) \n",
    "        - Start each robot at same configuration (can try this)\n",
    "        - increase granularity (make angle interval smaller)\n",
    "        - Try limiting theta (-pi/4, pi/4, pi/32)\n",
    "        - change the reward to prioritize theta_dot (so that it turns in circles)\n",
    "    3. Function approximation for x (if have time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/25\n",
    "\n",
    "- questions:\n",
    "    - how to resolve weird state parameters in RL?\n",
    "- To-do\n",
    "   - change state space to (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "   - when theta goes out of range, in move(), add or minus 2pi depending on whether it is positive or negative\n",
    "   - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "   - implement graphing of x-velocity, x displacement, joint angles etc. in policy testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/27\n",
    "\n",
    "- questions:\n",
    "    - is my model working correctly?\n",
    "- observation:\n",
    "    - time_interval is dependent on angle_interval, if t_interval is too small, and change in angle_interval is too small, there might not be any update\n",
    "    - afraid that this might be a problem?\n",
    "- Deep RL:\n",
    "    - loss function = expected reward - groundtruth reward\n",
    "- RL implementation:\n",
    "    - always use epsilon-greedy approach\n",
    "     - epsilon should be something that decreases over time\n",
    "    - (optional)in general, joint velocities are sinusoidal -> this is how we should let the robot choose \n",
    "       - Tony will think about this more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/6\n",
    "\n",
    "- next time\n",
    "    - look at Atari paper for Deep RL implementation details\n",
    "- questions:\n",
    "    - what should self.state contain?\n",
    "    - implement discretized angles?\n",
    "    - are mutator/accessor methods useless?\n",
    "    - Deep RL link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/30\n",
    "\n",
    "- next time\n",
    "    - fix DeepRobots implementation\n",
    "    - ask Tony for Mathematica inputs and outputs\n",
    "    - compare with Python model\n",
    "- questions:\n",
    "    - openAI (don't worry)\n",
    "    - how to keep the continuity Scott mentioned (don't worry)\n",
    "    - this implementation is only for proximal link, do we need other links? (implement the function)\n",
    "    - do I need to integrate over theta_dot? Is it already given by x and y dot? (No)\n",
    "    - when the input is pi/3, pi/3, D becomes 0 (singularity)\n",
    "    - self.state is broken (another version of implementation) \n",
    "    - plot graph with matplotlib (noted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
