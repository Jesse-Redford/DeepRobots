{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRobots Research Meeting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "\n",
    "### Discrete Implementation Details\n",
    "\n",
    "- state space (done)\n",
    "    - self.state\n",
    "    - (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "- action space\n",
    "    - joint velocities (adot1, adot2), range = (-pi/8, pi/8) not including the case where (adot1 = 0, adot2 = 0), intervals = 0.01\n",
    "    - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "- reward\n",
    "    - negative reward for singularity (a1 = a2) = -(link_length)*10\n",
    "    - negative reward based on proximity of a1 and a2 (using log function, log 0 = -inf)\n",
    "    - body_v[0] / (a1dot^2 + a2dot^2) * some constant for scaling\n",
    "        - displacement from current state to next state along x axis is x, x dot is velocity\n",
    "- transition model (done)\n",
    "- goal state (no goal state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "1. dicrete RL implementation\n",
    "2. Deep RL\n",
    "3. continuous implementation (python function sovler to solve ODE)\n",
    "4. DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/8\n",
    "\n",
    "- Updates\n",
    "    - implemented 3d graph of a1,a2 vs time\n",
    "        - ![title](images/Q_monitor.png)\n",
    "        - ![title](images/Q_3D_monitor.png)\n",
    "    - implement Fourier transformation\n",
    "        - ![title](images/Fourier Transformation.png)\n",
    "    \n",
    "- questions/problems:\n",
    "- To-do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2/1\n",
    "\n",
    "- questions/problems:\n",
    "    - apologize for cumulating the work until thursday, will try to work on weekends + thursday from now on\n",
    "    - q-learning sometimes stuck in an infinite loop choosing an action?\n",
    "    - does not cover enough states so cannot do policy roll out?\n",
    "- To-do\n",
    "    1. implement\n",
    "        - Plot trajectories alpha1-alpha2 space\n",
    "        - Extract modes from trajectories\n",
    "            - Fourier transformations\n",
    "    2. play around(optional, for fun) \n",
    "        - Start each robot at same configuration (can try this)\n",
    "        - increase granularity (make angle interval smaller)\n",
    "        - Try limiting theta (-pi/4, pi/4, pi/32)\n",
    "        - change the reward to prioritize theta_dot (so that it turns in circles)\n",
    "    3. Function approximation for x (if have time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/25\n",
    "\n",
    "- questions:\n",
    "    - how to resolve weird state parameters in RL?\n",
    "- To-do\n",
    "   - change state space to (theta, a1, a2), where theta is (-pi, pi) with pi/32 interval, a1 is (0, pi/8) with pi/32 interval, a2 is (-pi/8, 0) with pi/32 interval\n",
    "   - when theta goes out of range, in move(), add or minus 2pi depending on whether it is positive or negative\n",
    "   - when choosing a random action, in a while loop, check if the action would result in valid a1 and a2 values through integration, only break when it is valid\n",
    "   - implement graphing of x-velocity, x displacement, joint angles etc. in policy testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/27\n",
    "\n",
    "- questions:\n",
    "    - is my model working correctly?\n",
    "- observation:\n",
    "    - time_interval is dependent on angle_interval, if t_interval is too small, and change in angle_interval is too small, there might not be any update\n",
    "    - afraid that this might be a problem?\n",
    "- Deep RL:\n",
    "    - loss function = expected reward - groundtruth reward\n",
    "- RL implementation:\n",
    "    - always use epsilon-greedy approach\n",
    "     - epsilon should be something that decreases over time\n",
    "    - (optional)in general, joint velocities are sinusoidal -> this is how we should let the robot choose \n",
    "       - Tony will think about this more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/6\n",
    "\n",
    "- next time\n",
    "    - look at Atari paper for Deep RL implementation details\n",
    "- questions:\n",
    "    - what should self.state contain?\n",
    "    - implement discretized angles?\n",
    "    - are mutator/accessor methods useless?\n",
    "    - Deep RL link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/30\n",
    "\n",
    "- next time\n",
    "    - fix DeepRobots implementation\n",
    "    - ask Tony for Mathematica inputs and outputs\n",
    "    - compare with Python model\n",
    "- questions:\n",
    "    - openAI (don't worry)\n",
    "    - how to keep the continuity Scott mentioned (don't worry)\n",
    "    - this implementation is only for proximal link, do we need other links? (implement the function)\n",
    "    - do I need to integrate over theta_dot? Is it already given by x and y dot? (No)\n",
    "    - when the input is pi/3, pi/3, D becomes 0 (singularity)\n",
    "    - self.state is broken (another version of implementation) \n",
    "    - plot graph with matplotlib (noted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
